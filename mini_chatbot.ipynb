{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nawel-Bellil/AI---Deep-Learning--/blob/main/mini_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1V-LtpnNqgC6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgMFTYb9qfyD",
        "outputId": "d63790dd-bc7f-4cde-dd48-bb9547641b4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5V8IgsOiouCU"
      },
      "outputs": [],
      "source": [
        "from nbformat import read, write\n",
        "import nbformat\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/DL code213/mini chatbot.ipynb\", \"r\", encoding=\"utf-8\") as f:\n",
        "    nb = read(f, as_version=4)\n",
        "\n",
        "# Clean up widget metadata\n",
        "for cell in nb.cells:\n",
        "    if \"metadata\" in cell and \"widgets\" in cell[\"metadata\"]:\n",
        "        cell[\"metadata\"].pop(\"widgets\")\n",
        "\n",
        "# Save the cleaned notebook\n",
        "with open(\"cleaned_notebook.ipynb\", \"w\", encoding=\"utf-8\") as f:\n",
        "    write(nb, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FBgg-0ijdeE",
        "outputId": "ffe62ef7-72c9-4429-e3f5-529d52670ea9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilh_BNNMhFee",
        "outputId": "a7ea49a1-4a4f-421b-f19c-385749b22e4e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import requests\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import random\n",
        "import io\n",
        "from io import StringIO\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umprZ-YLlblR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0-hs8PVhXzF"
      },
      "source": [
        "# DATA PREPARATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1zRDbPOhV2d"
      },
      "outputs": [],
      "source": [
        "def download_data():\n",
        "    \"\"\"Download and prepare an advanced FAQ dataset\"\"\"\n",
        "    print(\"Downloading FAQ dataset...\")\n",
        "\n",
        "    # Using the StackExchange FAQ dataset\n",
        "    url = \"https://github.com/MrJay10/banking-faq-bot/blob/master/BankFAQs.csv\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        data = pd.read_csv(io.StringIO(response.text), on_pad_lines = 'skip')\n",
        "        print(f\"Dataset loaded with {len(data)} question-answer pairs\")\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading dataset: {e}\")\n",
        "        # Fallback to a small built-in dataset\n",
        "        return pd.DataFrame({\n",
        "            'Question': [\n",
        "                \"What is machine learning?\",\n",
        "                \"How do neural networks work?\",\n",
        "                \"What is natural language processing?\",\n",
        "                \"Explain what a chatbot is\",\n",
        "                \"What are embeddings in NLP?\",\n",
        "                \"How does sentiment analysis work?\",\n",
        "                \"What is transfer learning in AI?\",\n",
        "                \"Explain the difference between AI and ML\",\n",
        "                \"What is deep learning?\",\n",
        "                \"How to measure chatbot effectiveness?\"\n",
        "            ],\n",
        "            'Answer': [\n",
        "                \"Machine learning is a field of AI that enables computers to learn from data without explicit programming.\",\n",
        "                \"Neural networks are computing systems inspired by biological neural networks, consisting of nodes (neurons) arranged in layers that process and transform input data to produce output.\",\n",
        "                \"Natural Language Processing (NLP) is a field of AI focused on enabling computers to understand, interpret, and generate human language.\",\n",
        "                \"A chatbot is a software application that uses AI to conduct conversations with users through text or speech interfaces.\",\n",
        "                \"Embeddings in NLP are vector representations of words or sentences that capture semantic meanings, allowing machines to understand relationships between terms.\",\n",
        "                \"Sentiment analysis works by using NLP techniques to identify and extract subjective information from text, determining if the expressed opinion is positive, negative, or neutral.\",\n",
        "                \"Transfer learning is an ML technique where a model developed for one task is reused as the starting point for another related task, saving time and computational resources.\",\n",
        "                \"AI (Artificial Intelligence) is the broader concept of machines being able to carry out tasks intelligently, while ML (Machine Learning) is a specific subset focused on training machines to learn patterns from data.\",\n",
        "                \"Deep learning is a subset of machine learning that uses neural networks with many layers (deep neural networks) to analyze various factors with a structure similar to the human brain.\",\n",
        "                \"Chatbot effectiveness can be measured through metrics like task completion rate, conversation length, user satisfaction scores, and correct response rate.\"\n",
        "            ]\n",
        "        })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyLm70CbhkzB"
      },
      "source": [
        "# preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4uVC3VWhnwV"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"Clean and normalize text\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Ensure NLTK data is downloaded\n",
        "    try:\n",
        "        nltk.data.find('tokenizers/punkt')\n",
        "    except LookupError:\n",
        "        nltk.download('punkt', quiet=True)\n",
        "\n",
        "    try:\n",
        "        nltk.data.find('corpora/stopwords')\n",
        "    except LookupError:\n",
        "        nltk.download('stopwords', quiet=True)\n",
        "\n",
        "    try:\n",
        "        nltk.data.find('corpora/wordnet')\n",
        "    except LookupError:\n",
        "        nltk.download('wordnet', quiet=True)\n",
        "            # Download the 'punkt_tab' data if not already present\n",
        "    try:\n",
        "        nltk.data.find('tokenizers/punkt_tab') # Check if 'punkt_tab' data is present\n",
        "    except LookupError:\n",
        "        nltk.download('punkt_tab', quiet=True) # Download 'punkt_tab' if not found\n",
        "\n",
        "\n",
        "    # Lowercase and remove special characters\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text.lower())\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and lemmatize\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    cleaned_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "\n",
        "    return \" \".join(cleaned_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p54TyLW2hqPq"
      },
      "outputs": [],
      "source": [
        "def prepare_knowledge_base(data):\n",
        "    \"\"\"Process the dataset into a clean knowledge base\"\"\"\n",
        "    # Use original questions and answers but also add preprocessed versions\n",
        "    knowledge_base = []\n",
        "\n",
        "    for i, row in data.iterrows():\n",
        "        question = row['Question'] if isinstance(row['Question'], str) else \"\"\n",
        "        answer = row['Answer'] if isinstance(row['Answer'], str) else \"\"\n",
        "\n",
        "        if question and answer:\n",
        "            # Preprocess for better matching\n",
        "            processed_question = preprocess_text(question)\n",
        "\n",
        "            knowledge_base.append({\n",
        "                'original_question': question,\n",
        "                'processed_question': processed_question,\n",
        "                'answer': answer\n",
        "            })\n",
        "\n",
        "    print(f\"Knowledge base prepared with {len(knowledge_base)} entries\")\n",
        "    return knowledge_base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78LZMkmChtYb"
      },
      "outputs": [],
      "source": [
        "def save_knowledge_base(knowledge_base, filename=\"knowledge_base.json\"):\n",
        "    \"\"\"Save the knowledge base to a file\"\"\"\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(knowledge_base, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"Knowledge base saved to {filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rgATOL-hzcS"
      },
      "outputs": [],
      "source": [
        "def load_knowledge_base(filename=\"knowledge_base.json\"):\n",
        "    \"\"\"Load the knowledge base from a file\"\"\"\n",
        "    if os.path.exists(filename):\n",
        "        with open(filename, 'r', encoding='utf-8') as f:\n",
        "            knowledge_base = json.load(f)\n",
        "        print(f\"Knowledge base loaded with {len(knowledge_base)} entries\")\n",
        "        return knowledge_base\n",
        "    else:\n",
        "        print(f\"No knowledge base file found at {filename}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzLoFzeSh3zS"
      },
      "source": [
        "# EMBEDDING MODULE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZR-5s1Ph1LW"
      },
      "outputs": [],
      "source": [
        "def initialize_embedding_model():\n",
        "    \"\"\"Initialize the sentence embedding model\"\"\"\n",
        "    print(\"Initializing embedding model...\")\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YCW-oxph57d"
      },
      "outputs": [],
      "source": [
        "def compute_embeddings(texts, model):\n",
        "    \"\"\"Compute embeddings for a list of texts\"\"\"\n",
        "    return model.encode(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJhwIXH0h7bD"
      },
      "outputs": [],
      "source": [
        "def prepare_embeddings(knowledge_base, model):\n",
        "    \"\"\"Create embeddings for the knowledge base\"\"\"\n",
        "    questions = [item['processed_question'] for item in knowledge_base]\n",
        "    embeddings = compute_embeddings(questions, model)\n",
        "\n",
        "    # Add embeddings to knowledge base\n",
        "    for i, item in enumerate(knowledge_base):\n",
        "        item['embedding'] = embeddings[i].tolist()\n",
        "\n",
        "    print(\"Embeddings prepared for knowledge base\")\n",
        "    return knowledge_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_FKNV_zh-bV"
      },
      "source": [
        "# QUERY HANDLING MODULE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QC4nmkTLh9gG"
      },
      "outputs": [],
      "source": [
        "def preprocess_query(query):\n",
        "    \"\"\"Clean and prepare the user query\"\"\"\n",
        "    return preprocess_text(query)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nL31uNEiBh2"
      },
      "outputs": [],
      "source": [
        "def find_best_match(query, knowledge_base, model, threshold=0.6):\n",
        "    \"\"\"Find the best matching question for the query\"\"\"\n",
        "    # Preprocess the query\n",
        "    processed_query = preprocess_query(query)\n",
        "\n",
        "    # Compute embedding for the query\n",
        "    query_embedding = model.encode([processed_query])[0]\n",
        "\n",
        "    # Find the best match\n",
        "    best_match_idx = -1\n",
        "    best_match_score = -1\n",
        "\n",
        "    for i, item in enumerate(knowledge_base):\n",
        "        # Get embedding from knowledge base\n",
        "        item_embedding = np.array(item['embedding'])\n",
        "\n",
        "        # Calculate cosine similarity\n",
        "        similarity = cosine_similarity([query_embedding], [item_embedding])[0][0]\n",
        "\n",
        "        if similarity > best_match_score:\n",
        "            best_match_score = similarity\n",
        "            best_match_idx = i\n",
        "\n",
        "    # Return the best match if it exceeds the threshold\n",
        "    if best_match_score >= threshold and best_match_idx != -1:\n",
        "        return knowledge_base[best_match_idx]['answer'], best_match_score, knowledge_base[best_match_idx]['original_question']\n",
        "    else:\n",
        "        fallback_responses = [\n",
        "            \"I'm not sure I understand. Could you rephrase your question?\",\n",
        "            \"I don't have enough information to answer that properly.\",\n",
        "            \"That's an interesting question, but I'm not confident in my answer.\",\n",
        "            \"I'm still learning about that topic. Could you ask something else?\"\n",
        "        ]\n",
        "        return random.choice(fallback_responses), best_match_score, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_opmkbkWiGt_"
      },
      "source": [
        "# RESPONSE GENERATION MODULE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5GYNGXZiEc7"
      },
      "outputs": [],
      "source": [
        "def generate_response(query, knowledge_base, model):\n",
        "    \"\"\"Generate a response to the user query\"\"\"\n",
        "    answer, confidence, matched_question = find_best_match(query, knowledge_base, model)\n",
        "\n",
        "    response_data = {\n",
        "        'answer': answer,\n",
        "        'confidence': confidence,\n",
        "        'matched_question': matched_question\n",
        "    }\n",
        "\n",
        "    return response_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWoxdzO8iIuF"
      },
      "outputs": [],
      "source": [
        "def format_response(response_data):\n",
        "    \"\"\"Format the response data for display\"\"\"\n",
        "    answer = response_data['answer']\n",
        "    confidence = response_data['confidence']\n",
        "    matched_question = response_data['matched_question']\n",
        "\n",
        "    formatted_response = f\"{answer}\"\n",
        "\n",
        "    if matched_question and confidence >= 0.8:\n",
        "        formatted_response += f\"\\n\\n(I matched your question to: '{matched_question}')\"\n",
        "\n",
        "    return formatted_response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "micOrHk2iMmn"
      },
      "source": [
        "# KNOWLEDGE BASE EXPANSION MODULE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRIakEktiK3o"
      },
      "outputs": [],
      "source": [
        "def add_to_knowledge_base(question, answer, knowledge_base, model):\n",
        "    \"\"\"Add a new question-answer pair to the knowledge base\"\"\"\n",
        "    processed_question = preprocess_text(question)\n",
        "    embedding = model.encode([processed_question])[0].tolist()\n",
        "\n",
        "    new_entry = {\n",
        "        'original_question': question,\n",
        "        'processed_question': processed_question,\n",
        "        'answer': answer,\n",
        "        'embedding': embedding\n",
        "    }\n",
        "\n",
        "    knowledge_base.append(new_entry)\n",
        "    print(f\"Added new knowledge: '{question}'\")\n",
        "    return knowledge_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meY9qrRKiP14"
      },
      "source": [
        "# CHAT INTERFACE MODULE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCO24hZTiPJl"
      },
      "outputs": [],
      "source": [
        "def start_chat():\n",
        "    \"\"\"Main chat interface\"\"\"\n",
        "    print(\"Initializing chatbot...\")\n",
        "\n",
        "    # Step 1: Prepare data\n",
        "    data = download_data()\n",
        "    knowledge_base = prepare_knowledge_base(data)\n",
        "\n",
        "    # Step 2: Initialize embedding model\n",
        "    model = initialize_embedding_model()\n",
        "\n",
        "    # Step 3: Prepare embeddings\n",
        "    knowledge_base = prepare_embeddings(knowledge_base, model)\n",
        "\n",
        "    # Save knowledge base (optional)\n",
        "    save_knowledge_base(knowledge_base)\n",
        "\n",
        "    print(\"\\n=== Modular Chatbot Ready ===\")\n",
        "    print(\"Type 'exit' to end the conversation\")\n",
        "    print(\"Use 'learn: your question | your answer' to teach me something new\")\n",
        "    print(\"============================\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \").strip()\n",
        "\n",
        "        if user_input.lower() in ['exit', 'quit', 'bye']:\n",
        "            print(\"Chatbot: Goodbye!\")\n",
        "            break\n",
        "\n",
        "        if user_input.lower().startswith(\"learn:\"):\n",
        "            # Format: learn: question | answer\n",
        "            parts = user_input[6:].split(\"|\")\n",
        "            if len(parts) == 2:\n",
        "                question = parts[0].strip()\n",
        "                answer = parts[1].strip()\n",
        "                knowledge_base = add_to_knowledge_base(question, answer, knowledge_base, model)\n",
        "                print(\"Chatbot: Thanks! I've learned something new.\")\n",
        "                # Save the updated knowledge base\n",
        "                save_knowledge_base(knowledge_base)\n",
        "                continue\n",
        "\n",
        "        # Process query and generate response\n",
        "        response_data = generate_response(user_input, knowledge_base, model)\n",
        "        formatted_response = format_response(response_data)\n",
        "\n",
        "        print(f\"Chatbot: {formatted_response}\")\n",
        "        print(f\"(Confidence: {response_data['confidence']:.2f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brLZPTjliXFS"
      },
      "source": [
        "# ENTRY POINT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 858,
          "referenced_widgets": [
            "b13821ed9d6d4cbb8f0c2c2a8f64359a",
            "bf5e2acbed014becab0dfcccb640f1b6",
            "35d4214fec004d0dac2e890ce71af993",
            "0f57c4bf6f5b44a1b51db3302a3aca79",
            "71e18cf3759e43f79dffee7a7fbeebc4",
            "030c256c7334423d8276b3e31ed07b4f",
            "5b5f829959f244b9936b5d066b36e9b8",
            "665c4e0f55a34a8b8c4b0707d5e2dcd8",
            "0f08e681cf67449c9c6a8754813b4c7a",
            "ea76989e18f9425fa48e9b5d74d7715b",
            "b5472aa4127f485296e529cbc1444853",
            "34a9bad730684d15b2279adb56b05e48",
            "edbe409534af4b2fb2322fd00c09602b",
            "cba55303bae241efb3a6b9f2e7c6d4e5",
            "3afcf211d07549809feabb1ff6aeefca",
            "e44ff25996654fe0b3205e3248e691ed",
            "032b972c6a9c4f3a978d1cae087ddff3",
            "aeca00cf19e64f19a812142fe8a43d40",
            "58e347d08e1f417d866f6d1c96afb5bc",
            "c552ce4ba4e5479f920cab4af03b08ba",
            "a1d5249a881f42dd8bb5d5c8bd379783",
            "d01149a9aa8447889738920f8829a979",
            "b578abf2a65b44a586f3c85e5941a5cd",
            "1d8f8683f46f46fd873cd9fea23bb2da",
            "b86fed6be1d84e3db4d29b2d786f13bd",
            "f6ddc44d2b5a490ba365b79517ea1f2b",
            "34637945c9e841e38f3e6c4f79d6c027",
            "62f9fbe635f645b591e1b2e365e02b96",
            "81473b073ea342928d40f58be144b49f",
            "86db7136ab4f4e8190b76539926725e2",
            "47964f745bc54fe993e0c7f645f405bd",
            "b4dee949645f46f585ee374ca9cf8f80",
            "ce5f9dde75e24dac917f4c3350080f03",
            "3eee7d0b0026447fad58d257b9872124",
            "b3bc04145b3b49cbaa54a2c8eaf8ca8b",
            "1aa4beead85945d69dfd2d199942466d",
            "6e48af81415e471a9fadd22e44240189",
            "08457ddf8f974a578c3559e74b928e32",
            "6d4fc8b568e24bb8996a797bebd07c2f",
            "7a0c6cadd3814b9884ccd6072fa42592",
            "1dc66eca5d09490f955dbc24cb61fd9a",
            "dd4810a42c904a85a817154d2073e653",
            "b31d7aa9880d486eaccca14a1e64d77e",
            "0adad00e866a483b92d95f014e7f4803",
            "1a7db35fbd3349e8b398185b60e95e4d",
            "627de72deb0542d384c76c904f001e75",
            "12d298ced4d44962b44a36b36fe6661b",
            "eece5263a1ab4327a843d9dd3517c1dc",
            "4f4b8ae05f424c32a2a3be901c2c6445",
            "b1d7f3d5285244ce8d2f1368bf501315",
            "247c680b21084b3d810c3385c3725848",
            "941025247f9b4dd69f121121b8a28245",
            "bd05495da2864685bf6a800d5583a542",
            "c16bf56d6890491f89f48e40d771a492",
            "5610ff5517594bd3a93392ec3f75a704",
            "5faa280d514f4dc78a54effa92731536",
            "e9c7de40ed7b4288b07ad8521a5e00e9",
            "b647631899a24211978a1b1d7c2a8aa3",
            "a5897016c76843e09fdd3ae514db3227",
            "880fd15ff9474cd3bf16d41680574393",
            "1f802acc033b4585836126b1ae72a477",
            "362d7f202a8d4e7492e10e5da86ab329",
            "85ca8364c2f64b118642f322dceadc92",
            "4d4f16cc796744d99f6e692db507cb10",
            "f1e98b8d5e9f4972ac1c18c52b3dda6e",
            "a20838fd297b472f9d330cc7a1d23c27",
            "b7eeec892adb40f78b62a4a933e4eaf1",
            "c845019d6c0e40d5b79eeffd66963b8f",
            "e90b45b72afe4703ae99276a2e2461a2",
            "222b3f7bbc584bccae20476609b79aeb",
            "dee8be75d48f449ea164ede7d9c188eb",
            "7d303fca674a42908d98656ccb209211",
            "645e49c9d6ed431f8eeb9aab074b4a9b",
            "8304c7a4f71a45d3835048dd161b81c2",
            "57b7bdeeb4654d3197f139a6d4a6934c",
            "de59746349504ee2926cecabf263416c",
            "cc527eee6d7f40cda39a81c9f23932bb",
            "22f577b90cef44809d0d53c63e2a3de6",
            "9ab05205381f407aacc4bb01e9c43a9b",
            "f9ed7eaf7bf849b68f15aa39d321fd06",
            "056752690f7e4949b2a3fa8de2631ee9",
            "d8d1be9e20a34288a7a6a9031c0c52e7",
            "b397f861304349a1ac19de0e29c866fe",
            "6e47eba6f21c406f8dd4461b1eac3b00",
            "2c37307182054a2a9626a8bfedd18143",
            "179649fd1dc643fa868c1c3aaddaa51f",
            "a81c65a06bba47d39a2b7b8dcfaa047a",
            "3feef48e0fdb4938bb6d0d5ec5c0b2af",
            "4607208eb9d64abb8006e2e4b7c72977",
            "e9e9b24c96204553a5aab8532510b7aa",
            "a117d2e16fe34c14b6de3d3ea42f3dc5",
            "e1f9c2f05d9b426187cb4f98155b00c9",
            "7becf0b130b44c0cab6fa1aa67f3d242",
            "7e681cd20ef14843b6e51946a4e992a6",
            "a581b403dc214b96bb156a3d9403ef07",
            "96fc20f20fd2415a94dc33dffa146119",
            "f375263ed3ae44258aba1be77311a218",
            "746078263de7435aafa7893433ce51e4",
            "84c3c62b7fde49a88b1052ef7f385f64",
            "f7417c33b3af4a519c95c33677866248",
            "224c84ef0a32436fb65705977d002ca2",
            "5ed214e6c4de4e2bbf07cf983d43be33",
            "4f8b1ece28ff4311a175b017b9f4a842",
            "3471be82bf0c4396b8aa27b6ba6a4525",
            "fcaa6a1acac64a1f8e803360a5bf4371",
            "93f612896b3843db9246663207c3a531",
            "582612dacc7e47cab610145e9c860353",
            "24dbf003444642d096fce5c08aded6db",
            "083c3a5182f24a0587e695d7f1111576",
            "ffda7890f8b946b6816a9e5d4bbf52e7",
            "a2dee4070f664b87ba7c8178d0803bf9",
            "b3401f16d9064f2fa56a38794b7b3927",
            "ef81350b33ee4d48b502bee37bcb3d4d",
            "202fcf5f64764f1499634ebc3909b2db",
            "6a65cebb270f47e79ba9e09dc428790e",
            "93756947b1394f759f0af770e5a1f03a",
            "9f16bbe7d4414784af9e0ef69280bc3c",
            "b04743a33e0a4e36bdb8aeb5d58da109",
            "eeeaa002167c47438122de3ff3be2e8e",
            "abd25514e13c4823b0e63352ff49007d",
            "dd45a2a6e654477e956e5e93aa4fe6cd"
          ]
        },
        "id": "PWkJXF3piVMP",
        "outputId": "94c9da98-f74c-4ecc-e559-a1fb17110229"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing chatbot...\n",
            "Downloading FAQ dataset...\n",
            "Error downloading dataset: read_csv() got an unexpected keyword argument 'on_pad_lines'\n",
            "Knowledge base prepared with 10 entries\n",
            "Initializing embedding model...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b13821ed9d6d4cbb8f0c2c2a8f64359a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "34a9bad730684d15b2279adb56b05e48",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b578abf2a65b44a586f3c85e5941a5cd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3eee7d0b0026447fad58d257b9872124",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1a7db35fbd3349e8b398185b60e95e4d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5faa280d514f4dc78a54effa92731536",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b7eeec892adb40f78b62a4a933e4eaf1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "22f577b90cef44809d0d53c63e2a3de6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4607208eb9d64abb8006e2e4b7c72977",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f7417c33b3af4a519c95c33677866248",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a2dee4070f664b87ba7c8178d0803bf9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embeddings prepared for knowledge base\n",
            "Knowledge base saved to knowledge_base.json\n",
            "\n",
            "=== Modular Chatbot Ready ===\n",
            "Type 'exit' to end the conversation\n",
            "Use 'learn: your question | your answer' to teach me something new\n",
            "============================\n",
            "\n",
            "Chatbot: Neural networks are computing systems inspired by biological neural networks, consisting of nodes (neurons) arranged in layers that process and transform input data to produce output.\n",
            "\n",
            "(I matched your question to: 'How do neural networks work?')\n",
            "(Confidence: 0.84)\n",
            "You: exit\n",
            "Chatbot: Goodbye!\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    start_chat()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOMST7s3ackZ9Dgg9k8IQlu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}